\section{Discussion}
Machine learning technique motivates the development of the ITS community in recent years. In particular, many deep-learning-based models including the aforementioned modern neural networks are successively proposed to meet the challenging traffic prediction task. The design and formulation of these impressive predictive models usually relies on some assumptions or specific observations on the recorded traffic signal data, and thus forming diverse network architectures of such learning models in terms of their respective inductive biases. Specifically, early deep-learning-based efforts started with treating traffic prediction as a sequence learning issue. They further naturally employed canonical RNNs (e.g., LSTM) to build the roughly similar backbone of their deep neural network architectures. The backbone models the dynamics of traffic system by focusing on the temporal dependencies hidden inside traffic conditions. After that, other studies also paid attention to the influence of potential spatial dependencies among traffic conditions on dynamic traffic system. They successively exploited well-established CNNs or emerging GNNs to enrich the above backbone for elaborating their distinctive model architectures. These models (e.g., the above baseline methods) all deliver excellent results, and thus a unified framework, called Spatial-Temporal Graph Neural Networks (STGNNs) \cite{2019A}, is formed for modeling dynamic traffic systems based on the pioneering works.

We argue that, like other complex systems, a traffic system is a coupled traffic network consisting of various channels and nodes, where traffic signals interact and are coupled with each other. Specifically, the dynamics of traffic conditions are associated with a variety of traffic signals and external environment (such as weather data), which interact and are coupled with each other as well. By viewing a traffic system as a coupled traffic network with multi-sensor traffic signals interacting and coupled, we can further model couplings in multi-modal traffic data and external data. We notice that the current widely-adopted framework STGNNs implies its inductive bias of modeling traffic systems likewise. Those models following the framework prefer to merely focus on two kinds of acquainted dependency relations (i.e., spatial and temporal relations) coupled inside traffic conditions. And they use an alternating neural network forward computation manner to pick up the two relations in turn, so as to excavate their coupling relations (i.e., so-called spatio-temporal relations) indirectly. Compared with them, MS-GAT attempts to improve the model bias by (1) shedding light on another noteworthy dependency relations (i.e., channel relations) that are also coupled inside traffic conditions, and (2) conceiving a synchronous neural network forward computation manner to separately identify all possible dependency relations of concern to us and explicitly assess their coupling strengths with each other to predictions. Ultimately, the above experimental results have also verified the superiority of our inductive bias to modeling dynamic traffic conditions. 

Furthermore, we notice that a burgeoning neural network architecture, called Transformer \cite{vaswani2017attention}, has flourished in the field of nature language processing (NLP) in the past few years. This architecture different from CNNs, RNNs and GNNs, can provide the relationships between features of different input data via the attention mechanism. Its power in sequence learning issue inspires other communities (e.g., computer vision, CV) to investigate the use of Transformer for their specific tasks. Not surprisingly, the novel architecture has also attracted a few ITS researchers to explore its potential in traffic predictions. The most recent ASTGNN \cite{2021Learning} is such a impressive transformer-like predictive model. It leverages Transformer to conduct the forward computation of the STGNNs framework for long-term prediction, and elaborates its distinctive inductive bias to concretize the transformer-based encoder-decoder framework with valuable considerations (e.g., explicitly modeling the periodicity and spatial heterogeneity). 

By experimental comparisons on the PEMSD4 dataset, ASTGNN performs slightly better than MS-GAT on the metrics of MAE, MAPE and RMSE by 5.6\%, 7.9\% and 2.1\% respectively for the next 1-hour prediction task. Definitely, the transformer-based architecture of ASTGNN has played a great role for the learning ability of the transformer-like model to make long-term predictions. It is because that massive multi-head attentions in Transformer with global receptive fields as well as unique embedding mechanisms of relevant information can effectively assist those transformer-based models to excavate more significant features for the sequence-to-sequence task. Meanwhile, ASTGNN also reveals its own inductive bias of modeling the dynamics of traffic conditions across both spatial and temporal dimensions. Concretely, except for taking the transformer-like framework as its backbone, ASTGNN also deliberately devises two important modules: (1) a trend-aware self-attention module that enables the self-attention being aware of the local temporal context, and () a dynamic graph convolution module that models the spatial dependencies. Notwithstanding, in contrast to MS-GAT, this kind of transformer-like predictive model also faces extra challenges, including: (i) \textit{huge amount of calculation}. Due to complex learning mechanisms and overparametered networks in Transformer, those transformer-like predictive models are typically difficult to train and depend on a huge number of training data. For instance, on our above experimental setting with 2080TI, ASTGNN need to cost around 220s to conduct an epoch of training, while MS-GAT just costs about 100s for every epoch; (ii) \textit{insufficiency of parallel capability}. The transformer-like predictive models like ASTGNN are all equipped with a heavy decoder that performs in an auto-regressive manner. It means that every output sequence of their decoders is generated one by one, i.e., the current prediction relies on the previous result. Instead, the decoding process of MS-GAT is developed to be a simple regressor based on neural network, which produces output sequences in a one-step manner, and its encoding process is designed to be a structure with multiple independent branches, where each branch is responsible for identifying a specific dependency relation coupled inside traffic conditions (e.g., spatial, temporal or channel relations) and can be conveniently deployed on a separate computation resource (e.g., CPU or GPU) for achieving parallel acceleration. 

To sum up, owing to its own architectural characteristics, MS-GAT is more applicable for short-term traffic prediction, and also suitable for the case of deploying parallel settings for pursuing real-time effect. Moreover, inspirited by the newly proposed ASTGNN that paves the way for transferring the success of pure transformer model in NLP to the traffic prediction task in ITS, we are also thinking about applying Transformer to MS-GAT, e.g., attaching a transformer-based decoder in MS-GAT for more accurate long-term prediction. 
